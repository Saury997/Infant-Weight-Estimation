data:
  path: ../data/my_dataset.xlsx
  target_column: 出生体重
  feature_engineering: true
  binning: false
  log_transform: false
  standardize: true
  test_size: 0.2

training:
  k_folds: 5

ml_search:
  algorithms: [ "KNN", "RandomForest", "XGBoost", "Ridge", "Lasso", "ElasticNet",
                "SVR", "KernelRidge", "ExtraTrees", "GBDT", "LightGBM", "CatBoost" ]
  mode: "bayes"              # "random", "grid" or "bayes"
  random_iter: 60           # 若 mode=random 必填
  scoring: "neg_root_mean_squared_error"
  search_n_jobs: -1
  refit: true
  return_train_score: false
  save_best_model: true

  knn:
    base_params: {n_jobs: -1 }
    search_space:
      n_neighbors: [3,5,7,9,11]
      weights: ["uniform","distance"]
      metric: ["minkowski"]
      p: [1,2]

  random_forest:
    base_params: {random_state: 42, n_jobs: -1, criterion: "squared_error"}
    search_space:
      n_estimators: [200, 400, 800, 1200]
      max_depth: [null, 6, 10, 16]
      min_samples_split: [2, 4, 8]
      min_samples_leaf: [1, 2, 4, 8]
      max_features: ["sqrt", "log2", 0.6, 0.8, 1.0]
      bootstrap: [true, false]

  xgboost:
    base_params: {random_state: 42, objective: "reg:squarederror", tree_method: "auto"}
    search_space:
      n_estimators: [500, 1000, 2000, 4000]
      learning_rate: [0.01, 0.02, 0.03, 0.05, 0.1]
      max_depth: [3, 4, 5, 6, 8]
      min_child_weight: [1, 3, 5, 7]
      subsample: [0.6, 0.8, 1.0]
      colsample_bytree: [0.6, 0.8, 1.0]
      gamma: [0.0, 0.1, 0.3, 1.0]
      reg_alpha: [0.0, 0.5, 1.0, 5.0]
      reg_lambda: [0.0, 1.0, 5.0, 10.0]

  svr:
    base_params: {}
    search_space:
      kernel: ["rbf"]
      C: [0.5, 1.0, 2.0, 5.0]
      epsilon: [0.05, 0.1, 0.2]
      gamma: ["scale","auto"]

  extra_trees:
    base_params: {n_jobs: -1, random_state: 42}
    search_space:
      n_estimators: [300,500,800]
      max_depth: [null, 12, 16, 24]
      min_samples_split: [2,5,10]
      min_samples_leaf: [1,2,4]

  ridge:
    base_params: {random_state: 42}
    search_space:
      alpha: [0.1, 1.0, 5.0, 10.0]
      solver: ["auto"]

  lasso:
    base_params: {max_iter: 10000, random_state: 4 }
    search_space:
      alpha: [0.0005, 0.001, 0.005, 0.01]

  elasticnet:
    base_params: {max_iter: 10000, random_state: 42}
    search_space:
      alpha: [0.0005, 0.001, 0.005, 0.01]
      l1_ratio: [0.2, 0.5, 0.8]

  kernel_ridge:
    base_params: {}
    search_space:
      alpha: [0.1, 1.0, 5.0]
      kernel: ["rbf", "laplacian"]
      gamma: [0.01, 0.1, 1.0]

  gbdt:
    base_params: {random_state: 42}
    search_space:
      n_estimators: [200, 400, 600]
      learning_rate: [0.03, 0.05, 0.1]
      max_depth: [3, 4, 5]
  lightgbm:
    base_params: {random_state: 42, boosting_type: "gbdt", objective: "regression", metric: [ "rmse" ]}
    search_space:
      n_estimators: [500, 1000, 2000, 4000]
      learning_rate: [0.01, 0.02, 0.03, 0.05]
      num_leaves: [15, 31, 63]
      max_depth: [-1, 4, 6, 8]
      min_data_in_leaf: [20, 40, 60]
      feature_fraction: [0.7, 0.8, 0.9, 1.0]
      bagging_fraction: [0.6, 0.8, 1.0]
      bagging_freq: [0, 1, 3]
      lambda_l1: [0.0, 1.0, 5.0]
      lambda_l2: [0.0, 3.0, 10.0]
  catboost:
    base_params: {random_state: 42, loss_function: "RMSE", eval_metric: "RMSE", allow_writing_files: False, verbose: False}
    search_space:
      iterations: [1000, 3000, 5000]
      learning_rate: [0.01, 0.02, 0.03, 0.05]
      depth: [4, 6, 8]
      l2_leaf_reg: [1.0, 3.0, 10.0]
      bootstrap_type: ["Bayesian", "Bernoulli"]
      subsample: [0.6, 0.8, 1.0]
      rsm: [0.7, 0.8, 1.0]
      random_strength: [0.5, 1.0, 2.0]
      leaf_estimation_iterations: [1, 3, 5]

others:
  random_seed: 42
  save_root: ../experiments